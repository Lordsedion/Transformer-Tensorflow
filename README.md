## Implementation of the Attention Is All You Need paper on Machine Translation Task using Tensorflow

This repository contains an implementation of the groundbreaking paper "Attention is All You Need" in TensorFlow. The paper introduces the Transformer architecture, which has become a cornerstone in natural language processing and sequence-to-sequence tasks.


## Overview
The Transformer architecture, introduced in the paper, replaces traditional recurrent and convolutional networks with a self-attention mechanism, enabling the model to capture long-range dependencies and improve parallelization.

This implementation provides an intuitive approach to understanding and experimenting with the Transformer model. The code is organized into an encoder, decoder, attention mechanisms, and training pipelines.


## Requirements
TensorFlow 
NumPy
Pandas


### References
Vaswani, A., et al. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems (NeurIPS).
